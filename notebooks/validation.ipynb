{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = Path(r\"C:\\Users\\aryav\\OneDrive\\codeshenanigans\\ICJ 2021\\ICJ code\\data\")\n",
    "modelpath = Path(r'C:\\Users\\aryav\\OneDrive\\codeshenanigans\\ICJ 2021\\ICJ code\\models\\colortesting')\n",
    "\n",
    "M1 = keras.models.load_model(modelpath/'M1.h5')\n",
    "M2 = keras.models.load_model(modelpath/'M2.h5')\n",
    "M3 = keras.models.load_model(modelpath/'M3.h5')\n",
    "M4 = keras.models.load_model(modelpath/'M4.h5')\n",
    "M5 = keras.models.load_model(modelpath/'M5.h5')\n",
    "\n",
    "dataset = pd.read_csv(datapath/'RW1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_small = dataset.sample(frac=0.1, random_state=42)\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset['Unnamed: 14701']\n",
    "\n",
    "num_letters = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F',\n",
    "               6: 'G', 7: 'H', 8: 'I', 9: 'K',\n",
    "               10: 'L', 11: 'M', 12: 'N', 13: 'O', 14: 'P',\n",
    "               15: 'Q', 16: 'R', 17: 'S', 18: 'T', 19: 'U',\n",
    "               20: 'V', 21: 'W', 22: 'X', 23: 'Y'}\n",
    "\n",
    "y_test = dataset['label']\n",
    "del dataset['label']\n",
    "x_test = dataset.values\n",
    "x_test = x_test / 255\n",
    "x_test = x_test.reshape(-1, 70, 70, 3)\n",
    "lb = LabelBinarizer()\n",
    "y_test = lb.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model):\n",
    "\ty_pred = model.predict(x_test)\n",
    "\tprint('Accuracy: ' + str(round(accuracy_score(y_test.argmax(axis=1), y_pred.argmax(axis=1)) * 100, 2)) + '%')\n",
    "\tdel y_pred\n",
    "\n",
    "def test_precision(model):\n",
    "\ty_pred = model.predict(x_test)\n",
    "\tprint('Precision: ' + str(round(precision_score(y_test.argmax(axis=1), y_pred.argmax(axis=1), average='weighted'), 2)))\n",
    "\tdel y_pred\n",
    "\n",
    "def test_recall(model):\n",
    "\ty_pred = model.predict(x_test)\n",
    "\tprint('Recall: ' + str(round(recall_score(y_test.argmax(axis=1), y_pred.argmax(axis=1), average='weighted'), 2)))\n",
    "\tdel y_pred\n",
    "\n",
    "def test_f1(model):\t\n",
    "\ty_pred = model.predict(x_test)\n",
    "\tprint('F1: ' + str(round(f1_score(y_test.argmax(axis=1), y_pred.argmax(axis=1), average='weighted'), 2)))\n",
    "\tdel y_pred\n",
    "\n",
    "def test_roc_auc(model):\n",
    "\ty_pred = model.predict(x_test)\n",
    "\tprint('ROC AUC: ' + str(round(roc_auc_score(y_test, y_pred, average='weighted', multi_class='ovo'), 2)))\n",
    "\tdel y_pred\n",
    "\n",
    "def test_mean_and_std(model):\n",
    "\t#create multiple random samples of the test set\n",
    "\t#then compute the mean and standard deviation of all of the predictions' accuracies\n",
    "\n",
    "\taccuracies = []\n",
    "\n",
    "\tfor i in range(200):\n",
    "\t\t#sample 50 images from the test set\t\n",
    "\t\tsample = random.sample(range(0, len(x_test)), 50)\n",
    "\n",
    "\t\t#predict the labels of the sampled images\n",
    "\t\ty_pred = model.predict(x_test[sample])\n",
    "\n",
    "\t\t#compute the accuracy of the predictions\n",
    "\t\taccuracy = accuracy_score(y_test[sample].argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\t\taccuracies.append(accuracy)\n",
    "\n",
    "\t\tdel y_pred\n",
    "\t\tdel sample\n",
    "\n",
    "\t#compute and print the mean and standard deviation of the accuracies\n",
    "\tprint('Mean: ' + str(round(np.mean(accuracies) * 100, 2)) + '%')\n",
    "\tprint('Standard Deviation: ' + str(round(np.std(accuracies) * 100, 2)) + '%')\n",
    "\n",
    "\n",
    "def make_cm(model):\n",
    "\ty_pred = model.predict(x_test)\n",
    "\tcm = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\tcm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\tcm = pd.DataFrame(cm, index=num_letters.values(), columns=num_letters.values())\n",
    "\tplt.figure(figsize=(10, 10))\n",
    "\tsns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "\tplt.ylabel('Actual')\n",
    "\tplt.xlabel('Predicted')\n",
    "\tplt.show()\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running accuracy tests...\n",
      "M1:\n",
      "Accuracy: 10.4%\n",
      "M2:\n",
      "Accuracy: 16.09%\n",
      "M3:\n",
      "Accuracy: 11.47%\n",
      "M4:\n",
      "Accuracy: 20.71%\n",
      "M5:\n",
      "Accuracy: 14.75%\n"
     ]
    }
   ],
   "source": [
    "print(\"Running accuracy tests...\")\n",
    "\n",
    "print(\"M1:\")\n",
    "test_accuracy(M1)\n",
    "print(\"M2:\")\n",
    "test_accuracy(M2)\n",
    "print(\"M3:\")\n",
    "test_accuracy(M3)\n",
    "print(\"M4:\")\n",
    "test_accuracy(M4)\n",
    "print(\"M5:\")\n",
    "test_accuracy(M5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running precision tests...\n",
      "M1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryav\\.conda\\envs\\tensor\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.21\n",
      "M2:\n",
      "Precision: 0.33\n",
      "M3:\n",
      "Precision: 0.28\n",
      "M4:\n",
      "Precision: 0.34\n",
      "M5:\n",
      "Precision: 0.22\n"
     ]
    }
   ],
   "source": [
    "print(\"Running precision tests...\")\n",
    "\n",
    "print(\"M1:\")\n",
    "test_precision(M1)\n",
    "print(\"M2:\")\n",
    "test_precision(M2)\n",
    "print(\"M3:\")\n",
    "test_precision(M3)\n",
    "print(\"M4:\")\n",
    "test_precision(M4)\n",
    "print(\"M5:\")\n",
    "test_precision(M5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running recall tests...\n",
      "M1:\n",
      "Recall: 0.1\n",
      "M2:\n",
      "Recall: 0.16\n",
      "M3:\n",
      "Recall: 0.11\n",
      "M4:\n",
      "Recall: 0.21\n",
      "M5:\n",
      "Recall: 0.15\n"
     ]
    }
   ],
   "source": [
    "print(\"Running recall tests...\")\n",
    "\n",
    "print(\"M1:\")\n",
    "test_recall(M1)\n",
    "print(\"M2:\")\n",
    "test_recall(M2)\n",
    "print(\"M3:\")\n",
    "test_recall(M3)\n",
    "print(\"M4:\")\n",
    "test_recall(M4)\n",
    "print(\"M5:\")\n",
    "test_recall(M5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running F1 score tests...\n",
      "M1:\n",
      "F1: 0.11\n",
      "M2:\n",
      "F1: 0.16\n",
      "M3:\n",
      "F1: 0.12\n",
      "M4:\n",
      "F1: 0.19\n",
      "M5:\n",
      "F1: 0.14\n"
     ]
    }
   ],
   "source": [
    "print(\"Running F1 score tests...\")\n",
    "\n",
    "print(\"M1:\")\n",
    "test_f1(M1)\n",
    "print(\"M2:\")\n",
    "test_f1(M2)\n",
    "print(\"M3:\")\n",
    "test_f1(M3)\n",
    "print(\"M4:\")\n",
    "test_f1(M4)\n",
    "print(\"M5:\")\n",
    "test_f1(M5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running AUC tests...\n",
      "M1:\n",
      "ROC AUC: 0.65\n",
      "M2:\n",
      "ROC AUC: 0.73\n",
      "M3:\n",
      "ROC AUC: 0.64\n",
      "M4:\n",
      "ROC AUC: 0.75\n",
      "M5:\n",
      "ROC AUC: 0.72\n"
     ]
    }
   ],
   "source": [
    "print(\"Running AUC tests...\")\n",
    "\n",
    "print(\"M1:\")\n",
    "test_roc_auc(M1)\n",
    "print(\"M2:\")\n",
    "test_roc_auc(M2)\n",
    "print(\"M3:\")\n",
    "test_roc_auc(M3)\n",
    "print(\"M4:\")\n",
    "test_roc_auc(M4)\n",
    "print(\"M5:\")\n",
    "test_roc_auc(M5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running mean and standard deviation tests...\n",
      "M1:\n",
      "Mean: 10.24%\n",
      "Standard Deviation: 4.36%\n",
      "M2:\n",
      "Mean: 15.63%\n",
      "Standard Deviation: 4.94%\n",
      "M3:\n",
      "Mean: 11.71%\n",
      "Standard Deviation: 4.17%\n",
      "M4:\n",
      "Mean: 20.08%\n",
      "Standard Deviation: 6.0%\n",
      "M5:\n",
      "Mean: 14.55%\n",
      "Standard Deviation: 4.49%\n"
     ]
    }
   ],
   "source": [
    "print (\"Running mean and standard deviation tests...\")\n",
    "\n",
    "print(\"M1:\")\n",
    "test_mean_and_std(M1)\n",
    "print(\"M2:\")\n",
    "test_mean_and_std(M2)\n",
    "print(\"M3:\")\n",
    "test_mean_and_std(M3)\n",
    "print(\"M4:\")\n",
    "test_mean_and_std(M4)\n",
    "print(\"M5:\")\n",
    "test_mean_and_std(M5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "860ded9df05bd2a7a6fb8b4625a47831bf412219bd320bf30090506a6002257a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
